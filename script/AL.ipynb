{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b590b3-9ea9-4430-b78d-18d15b0175ee",
   "metadata": {},
   "source": [
    "# AI Language Model Interface Documentation\n",
    "\n",
    "## Introduction\n",
    "AL simplifies the complexity of large language models into a simple, open-source solution. One intuitive interface. Multiple AI powerhouses. Ollama, Anthropic, OpenAI - seamlessly integrated into all python environments, you can use it even on 1970 terminals if youd like that sort of a thing.\n",
    "This framework simplifies interactions with large language models through a consistent, open-source interface. It provides seamless integration with multiple AI providers (Ollama, Anthropic, OpenAI) in any Python environment, from modern IDEs to basic terminals.\n",
    "\n",
    "The interface preserves the complete interaction history, captures code snippets, and integrates smoothly with Jupyter notebooks. This enables easy sharing, versioning, and automation of AI workflows. The saved sessions serve as valuable resources for:\n",
    "- Model fine-tuning\n",
    "- Workflow iteration\n",
    "- Learning and training\n",
    "- Pattern analysis\n",
    "- Response comparison across models\n",
    "\n",
    "Key Features:\n",
    "* Simple markdown/ASCII interface\n",
    "* Flexible model selection\n",
    "* Conversation history management\n",
    "* Code snippet extraction and management\n",
    "* Jupyter notebook integration\n",
    "\n",
    "## Why Use This Interface?\n",
    "\n",
    "1. **Unified API**: Consistent interaction patterns across different AI models\n",
    "2. **Modular Design**: Easy to extend and customize for new AI services\n",
    "3. **Built-in History**: Automatic conversation tracking and management\n",
    "4. **Code Management**: Automatic extraction and organization of code snippets\n",
    "5. **Notebook Support**: Native integration with Jupyter environments\n",
    "\n",
    "## Class Structure\n",
    "\n",
    "### Base Class: AILanguageModel\n",
    "\n",
    "The `AILanguageModel` serves as the abstract base class defining the core interface:\n",
    "\n",
    "```python\n",
    "class AILanguageModel:\n",
    "    def __init__(self, model: str):\n",
    "        # Initialize model configuration\n",
    "    \n",
    "    def send_message(self, message: str) -> None:\n",
    "        # Send message and handle response\n",
    "    \n",
    "    def save_code(self, index: Optional[int] = None) -> None:\n",
    "        # Save code snippet to clipboard\n",
    "```\n",
    "\n",
    "### Implementation Classes\n",
    "\n",
    "Each AI service has its own implementation:\n",
    "\n",
    "```python\n",
    "class OllamaModel(AILanguageModel):\n",
    "    def __init__(self, model: str = \"llama3.1\"):\n",
    "        # Ollama-specific initialization\n",
    "\n",
    "class ClaudeModel(AILanguageModel):\n",
    "    def __init__(self, model: str, api_key: str):\n",
    "        # Claude-specific initialization\n",
    "\n",
    "class ChatGPTModel(AILanguageModel):\n",
    "    def __init__(self, model: str, api_key: str):\n",
    "        # ChatGPT-specific initialization\n",
    "```\n",
    "\n",
    "## Usage Examples\n",
    "\n",
    "### Basic Usage\n",
    "\n",
    "```python\n",
    "# Using Ollama\n",
    "llm = OllamaModel(\"llama3.1\")\n",
    "llm.send_message(\"Write a quicksort implementation\")\n",
    "\n",
    "# Using Claude\n",
    "claude = ClaudeModel(\"claude-3\", \"your-api-key\")\n",
    "claude.send_message(\"Explain quantum computing\")\n",
    "\n",
    "# Using ChatGPT\n",
    "gpt = ChatGPTModel(\"gpt-4\", \"your-api-key\")\n",
    "gpt.send_message(\"Create a React component\")\n",
    "```\n",
    "\n",
    "### Managing Code Snippets\n",
    "\n",
    "```python\n",
    "# Save the most recent code snippet\n",
    "llm.save_code()\n",
    "\n",
    "# Save a specific code snippet by index\n",
    "llm.save_code(1)  # Saves the second-most-recent snippet\n",
    "```\n",
    "\n",
    "## Extending the System\n",
    "\n",
    "To add support for a new AI service:\n",
    "\n",
    "1. Create a new class inheriting from `AILanguageModel`:\n",
    "\n",
    "```python\n",
    "class NewAIModel(AILanguageModel):\n",
    "    \"\"\"\n",
    "    Implementation for a new AI service with streaming support\n",
    "    \n",
    "    Args:\n",
    "        model (str): Name or identifier of the model to use\n",
    "        api_key (str): API key for the service\n",
    "        stream (bool): Whether to stream responses (default: True)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str, api_key: str, stream: bool = True):\n",
    "        super().__init__(model, stream)\n",
    "        # Initialize your API client\n",
    "        self.client = YourAPIClient(api_key)\n",
    "\n",
    "    def _send_to_model(self, messages: List[Dict]) -> Union[str, Generator[str, None, None]]:\n",
    "        \"\"\"\n",
    "        Send messages to the model and get response\n",
    "        \n",
    "        Args:\n",
    "            messages: List of conversation messages\n",
    "            \n",
    "        Returns:\n",
    "            Union[str, Generator[str, None, None]]: Either complete response string or\n",
    "                                                   generator yielding response chunks\n",
    "        \"\"\"\n",
    "        if self.stream:\n",
    "            return self._stream_response(messages)\n",
    "        else:\n",
    "            # Implement non-streaming API call\n",
    "            response = self.client.send_message(messages)\n",
    "            return response.text\n",
    "    \n",
    "    def _stream_response(self, messages: List[Dict]) -> Generator[str, None, None]:\n",
    "        \"\"\"\n",
    "        Stream response from the model\n",
    "        \n",
    "        Args:\n",
    "            messages: List of conversation messages\n",
    "            \n",
    "        Yields:\n",
    "            str: Response text chunks\n",
    "        \"\"\"\n",
    "        # Implement streaming API call\n",
    "        for chunk in self.client.stream_message(messages):\n",
    "            if chunk.has_content:\n",
    "                yield chunk.text\n",
    "```\n",
    "\n",
    "2. Key methods to implement:\n",
    "   - `__init__`: Set up your model and API client\n",
    "   - `_send_to_model`: Handle the actual API communication\n",
    "   - `_stream_response`: For streaming support\n",
    "\n",
    "3. Optional overrides:\n",
    "   - `_extract_code_blocks`: If your model returns code in a different format\n",
    "   - `_add_to_history`: If you need custom history management\n",
    "   - \n",
    "4. Response Processing:\n",
    "   - For streaming: ```yield``` text chunks ```as``` they arrive\n",
    "   - For non-streaming: ```return``` complete response text\n",
    "   - Ensure consistent markdown formatting\n",
    "\n",
    "## Core Components\n",
    "\n",
    "### Message Handling\n",
    "- `send_message`: Primary method for sending user input\n",
    "- `_send_to_model`: Model-specific API communication\n",
    "- `_add_to_history`: Conversation history management\n",
    "\n",
    "### Code Management\n",
    "- `save_code`: Save code snippets to clipboard\n",
    "- `_extract_code_blocks`: Parse code from responses\n",
    "\n",
    "### History Management\n",
    "- `conversation_history`: Stores all interactions\n",
    "- `code_snippets`: Maintains extracted code blocks\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Model Selection**:\n",
    "   - Choose the appropriate model class for your use case\n",
    "   - Consider rate limits and API costs\n",
    "\n",
    "2. **Error Handling**:\n",
    "   - Implement proper API error handling\n",
    "   - Monitor API quota usage\n",
    "\n",
    "3. **History Management**:\n",
    "   - Regularly save important conversations\n",
    "   - Clear history for sensitive information\n",
    "\n",
    "4. **Extension Development**:\n",
    "   - Follow the established pattern for new models\n",
    "   - Maintain consistent interface behavior\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This framework provides a robust foundation for AI model interactions. Its modular design allows easy integration of new services while maintaining a consistent interface. The built-in features for history management and code handling make it particularly valuable for development workflows and educational contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2a3421fc-6d5e-46b5-8031-fdf890179c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Generator, Union\n",
    "from IPython.display import Markdown, display\n",
    "from datetime import datetime\n",
    "import pyperclip\n",
    "from abc import ABC, abstractmethod\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class AILanguageModel(ABC):\n",
    "    \"\"\"Base class for AI language model interactions with streaming support\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str, stream: bool = True):\n",
    "        self.model = model\n",
    "        self.stream = stream\n",
    "        self.conversation_history = []\n",
    "        self.code_snippets = []\n",
    "        self.current_snippet_index = 0\n",
    "        self._current_response = \"\"\n",
    "\n",
    "    def send_message(self, message: str) -> None:\n",
    "        \"\"\"Send a message to the AI model and display its response\"\"\"\n",
    "        # Record user message\n",
    "        self._add_to_history(\"user\", message)\n",
    "        \n",
    "        # Get response\n",
    "        if self.stream:\n",
    "            self._current_response = \"\"\n",
    "            for chunk in self._send_to_model(self.conversation_history):\n",
    "                self._current_response += chunk\n",
    "                # Clear previous output and display updated response\n",
    "                clear_output(wait=True)\n",
    "                display(Markdown(self._current_response))\n",
    "            \n",
    "            response = self._current_response\n",
    "        else:\n",
    "            response = self._send_to_model(self.conversation_history)\n",
    "            display(Markdown(response))\n",
    "        \n",
    "        # Process response after completion\n",
    "        self._extract_code_blocks(response)\n",
    "        self._add_to_history(\"assistant\", response)\n",
    "\n",
    "    def save_code(self, index: Optional[int] = None) -> None:\n",
    "        \"\"\"Save a code snippet to clipboard\"\"\"\n",
    "        index = index if index is not None else self.current_snippet_index\n",
    "        \n",
    "        if 0 <= index < len(self.code_snippets):\n",
    "            code = self.code_snippets[index]\n",
    "            pyperclip.copy(code)\n",
    "            print(f\"Saved code snippet {index} to clipboard:\")\n",
    "            print(code)\n",
    "        else:\n",
    "            print(\"Invalid code snippet index\")\n",
    "\n",
    "    def _add_to_history(self, role: str, content: str) -> None:\n",
    "        \"\"\"Add a message to conversation history\"\"\"\n",
    "        self.conversation_history.append({\n",
    "            \"role\": role,\n",
    "            \"content\": content,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "    def _extract_code_blocks(self, text: str) -> None:\n",
    "        \"\"\"Extract code blocks from markdown-formatted text\"\"\"\n",
    "        parts = text.split('```')\n",
    "        for i, part in enumerate(parts):\n",
    "            if i % 2 == 1:  # Code block\n",
    "                lines = part.splitlines()\n",
    "                if lines:\n",
    "                    # Remove language identifier from first line\n",
    "                    code = '\\n'.join(lines[1:]).strip()\n",
    "                    self.code_snippets.insert(0, code)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _send_to_model(self, messages: List[Dict]) -> Union[str, Generator[str, None, None]]:\n",
    "        \"\"\"\n",
    "        Send messages to model and get response\n",
    "        \n",
    "        Args:\n",
    "            messages: List of message dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            Union[str, Generator[str, None, None]]: Either complete response string or\n",
    "                                                   generator yielding response chunks\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4f8a1b-25b0-40a0-9f05-193d357dbced",
   "metadata": {},
   "source": [
    "# Jupyter Notebook Integration Guide\n",
    "\n",
    "## Overview\n",
    "\n",
    "AL doesn't just fetch answers; it preserves your learning journey. Every interaction, every spark of inspiration, is captured in Jupyter notebooks. Share, version, and automate your AI workflows with ease.\n",
    "The Jupyter notebook integration enables seamless conversation management through `.ipynb` files. This functionality serves two primary purposes:\n",
    "1. Creating fine-tuning datasets by editing and refining conversations\n",
    "2. Building template conversations for automated responses\n",
    "\n",
    "## Features\n",
    "\n",
    "### Conversation Export\n",
    "- Preserves complete conversation history\n",
    "- Maintains markdown formatting\n",
    "- Separates code blocks into executable cells\n",
    "- Retains role information (user/assistant)\n",
    "\n",
    "### Conversation Import\n",
    "- Loads existing conversations\n",
    "- Reconstructs conversation history\n",
    "- Extracts code snippets automatically\n",
    "- Validates conversation structure\n",
    "\n",
    "## Usage Examples\n",
    "\n",
    "### Exporting Conversations\n",
    "\n",
    "```python\n",
    "# Create and use your model\n",
    "model = AILanguageModel(\"your-model\")\n",
    "model.send_message(\"Hello, can you explain Python generators?\")\n",
    "\n",
    "# Export the conversation\n",
    "model.export_to_notebook(\"python_explanation.ipynb\")\n",
    "```\n",
    "\n",
    "### Importing and Modifying Conversations\n",
    "\n",
    "```python\n",
    "# Load an existing conversation\n",
    "model = AILanguageModel(\"your-model\")\n",
    "model.import_from_notebook(\"template_responses.ipynb\")\n",
    "\n",
    "# Continue the conversation\n",
    "model.send_message(\"Can you elaborate on the last point?\")\n",
    "```\n",
    "\n",
    "## Creating Fine-tuning Datasets\n",
    "\n",
    "The notebook format allows you to create and edit training data efficiently:\n",
    "\n",
    "1. **Record Initial Conversations**\n",
    "   ```python\n",
    "   model.send_message(\"Your query\")\n",
    "   model.export_to_notebook(\"training_data.ipynb\")\n",
    "   ```\n",
    "\n",
    "2. **Edit in Jupyter Lab**\n",
    "   - Open the notebook in Jupyter Lab\n",
    "   - Modify responses for better quality\n",
    "   - Add alternative phrasings\n",
    "   - Remove irrelevant parts\n",
    "   - Add more context or examples\n",
    "\n",
    "3. **Import Modified Conversation**\n",
    "   ```python\n",
    "   model.import_from_notebook(\"improved_training_data.ipynb\")\n",
    "   ```\n",
    "\n",
    "## Building Template Conversations\n",
    "\n",
    "Create reusable conversation templates for common scenarios:\n",
    "\n",
    "1. **Create Base Template**\n",
    "   ```python\n",
    "   # Record template conversation\n",
    "   model.send_message(\"Template query with placeholders\")\n",
    "   model.export_to_notebook(\"template.ipynb\")\n",
    "   ```\n",
    "\n",
    "2. **Customize in Jupyter Lab**\n",
    "   - Add placeholder markers\n",
    "   - Include alternative responses\n",
    "   - Add conditional sections\n",
    "   - Document usage instructions\n",
    "\n",
    "3. **Use as Response Template**\n",
    "   ```python\n",
    "   # Load template and customize\n",
    "   model.import_from_notebook(\"customer_support_template.ipynb\")\n",
    "   ```\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "### Cell Types\n",
    "1. **Role Headers**\n",
    "   - Markdown cells starting with `## User` or `## Assistant`\n",
    "   - Indicate the speaker for following content\n",
    "\n",
    "2. **Content Cells**\n",
    "   - Markdown cells for regular text\n",
    "   - Code cells for executable code\n",
    "   - Preserves formatting and structure\n",
    "\n",
    "## Tips for Effective Use\n",
    "\n",
    "1. **Fine-tuning**\n",
    "   - Start with real conversations\n",
    "   - Edit for clarity and correctness\n",
    "   - Add alternative phrasings\n",
    "   - Include edge cases\n",
    "\n",
    "2. **Templates**\n",
    "   - Use clear placeholder markers\n",
    "   - Include usage instructions\n",
    "   - Document customization points\n",
    "   - Provide examples\n",
    "\n",
    "3. **Version Control**\n",
    "   - Keep backup copies\n",
    "   - Track changes\n",
    "   - Document modifications\n",
    "   - Test before deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "31400536-64b8-483e-9516-9d28049bb52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict\n",
    "import nbformat as nbf\n",
    "from datetime import datetime\n",
    "from IPython.display import Markdown\n",
    "\n",
    "class AILanguageModel(AILanguageModel):\n",
    "    \"\"\"AILanguageModel with Jupyter notebook support for saving and loading conversations\"\"\"\n",
    "    \n",
    "    def export_to_notebook(self, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Save conversation history to a Jupyter notebook\n",
    "        \n",
    "        Args:\n",
    "            filename (str): Target notebook filename (.ipynb extension optional)\n",
    "        \"\"\"\n",
    "        if not filename.endswith('.ipynb'):\n",
    "            filename += '.ipynb'\n",
    "            \n",
    "        notebook = nbf.v4.new_notebook()\n",
    "        cells = []\n",
    "        \n",
    "        for entry in self.conversation_history:\n",
    "            # Add role header\n",
    "            cells.append(nbf.v4.new_markdown_cell(\n",
    "                f\"## {entry['role'].capitalize()}\"\n",
    "            ))\n",
    "            \n",
    "            # Split content into code and markdown\n",
    "            parts = entry['content'].split('```')\n",
    "            \n",
    "            for i, part in enumerate(parts):\n",
    "                if i % 2 == 0 and part.strip():  # Markdown content\n",
    "                    cells.append(nbf.v4.new_markdown_cell(part.strip()))\n",
    "                elif i % 2 == 1:  # Code content\n",
    "                    lines = part.splitlines()\n",
    "                    if lines:\n",
    "                        code = '\\n'.join(lines[1:]).strip()  # Skip language identifier\n",
    "                        cells.append(nbf.v4.new_code_cell(code))\n",
    "                    \n",
    "        notebook['cells'] = cells\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            nbf.write(notebook, f)\n",
    "            \n",
    "        print(f\"Conversation exported to {filename}\")\n",
    "    \n",
    "    def import_from_notebook(self, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Load conversation history from a Jupyter notebook\n",
    "        \n",
    "        Args:\n",
    "            filename (str): Source notebook filename (.ipynb extension optional)\n",
    "        \"\"\"\n",
    "        if not filename.endswith('.ipynb'):\n",
    "            filename += '.ipynb'\n",
    "            \n",
    "        # Read notebook\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            notebook = nbf.read(f, as_version=4)\n",
    "        \n",
    "        # Clear existing history\n",
    "        self.conversation_history = []\n",
    "        self.code_snippets = []\n",
    "        \n",
    "        current_entry = None\n",
    "        \n",
    "        # Process cells\n",
    "        for cell in notebook.cells:\n",
    "            source = cell['source']\n",
    "            \n",
    "            if cell['cell_type'] == 'markdown' and source.startswith('## '):\n",
    "                # Save previous entry if exists\n",
    "                if current_entry:\n",
    "                    self.conversation_history.append(current_entry)\n",
    "                \n",
    "                # Start new entry\n",
    "                role = source[3:].lower().strip()\n",
    "                current_entry = {\n",
    "                    'role': role,\n",
    "                    'content': '',\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "            \n",
    "            elif current_entry:\n",
    "                # Add content to current entry\n",
    "                if cell['cell_type'] == 'code':\n",
    "                    if current_entry['role'] == 'assistant':\n",
    "                        self.code_snippets.insert(0, source)\n",
    "                    current_entry['content'] += f'```python\\n{source}\\n```\\n'\n",
    "                else:\n",
    "                    current_entry['content'] += f'{source}\\n'\n",
    "        \n",
    "        # Add final entry if exists\n",
    "        if current_entry:\n",
    "            self.conversation_history.append(current_entry)\n",
    "            \n",
    "        print(f\"Conversation imported from {filename}\")\n",
    "        print(f\"Loaded {len(self.conversation_history)} messages and {len(self.code_snippets)} code snippets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "bc420f7c-2e0e-4fed-8a59-1b1b43bdd7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union, Generator\n",
    "import ollama\n",
    "\n",
    "class OllamaModel(AILanguageModel):\n",
    "    \"\"\"\n",
    "    Ollama-specific implementation with streaming support\n",
    "    \n",
    "    Args:\n",
    "        model (str): Name of the Ollama model to use\n",
    "        stream (bool): Whether to stream responses (default: True)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"llama3.1\", stream: bool = True):\n",
    "        super().__init__(model, stream)\n",
    "        self.client = ollama\n",
    "\n",
    "    def _send_to_model(self, messages: List[Dict]) -> Union[str, Generator[str, None, None]]:\n",
    "        if self.stream:\n",
    "            return self._stream_response(messages)\n",
    "        else:\n",
    "            response = self.client.chat(\n",
    "                model=self.model,\n",
    "                messages=messages\n",
    "            )\n",
    "            return response['message']['content']\n",
    "    \n",
    "    def _stream_response(self, messages: List[Dict]) -> Generator[str, None, None]:\n",
    "        #Stream response from Ollama model\n",
    "        for chunk in self.client.chat(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        ):\n",
    "            if 'message' in chunk and 'content' in chunk['message']:\n",
    "                yield chunk['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafc0423-5342-4b66-9fe5-2c82fc3f892f",
   "metadata": {},
   "source": [
    "Intrface for Antropic Claude, currently untested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ef1ab-41e6-43e2-ae2d-b8f95c520a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union, Generator\n",
    "from anthropic import Anthropic\n",
    "\n",
    "class ClaudeModel(AILanguageModel):\n",
    "    \"\"\"\n",
    "    Anthropic Claude-specific implementation with streaming support\n",
    "    \n",
    "    Args:\n",
    "        model (str): Name of the Claude model to use\n",
    "        api_key (str): Anthropic API key\n",
    "        stream (bool): Whether to stream responses (default: True)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str, api_key: str, stream: bool = True):\n",
    "        super().__init__(model, stream)\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "\n",
    "    def _send_to_model(self, messages: List[Dict]) -> Union[str, Generator[str, None, None]]:\n",
    "        if self.stream:\n",
    "            return self._stream_response(messages)\n",
    "        else:\n",
    "            response = self.client.messages.create(\n",
    "                model=self.model,\n",
    "                messages=messages\n",
    "            )\n",
    "            return response.content[0].text\n",
    "    \n",
    "    def _stream_response(self, messages: List[Dict]) -> Generator[str, None, None]:\n",
    "        #Stream response from Claude model\n",
    "        with self.client.messages.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        ) as stream:\n",
    "            for chunk in stream:\n",
    "                if chunk.type == \"content_block_delta\":\n",
    "                    yield chunk.delta.text\n",
    "                elif chunk.type == \"message_delta\":\n",
    "                    continue\n",
    "                elif chunk.type == \"error\":\n",
    "                    raise Exception(f\"Error in stream: {chunk.error}\")\n",
    "                else:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daf8a3a-2c3c-4a8f-a8c0-9b56d7a61b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union, Generator\n",
    "from openai import OpenAI\n",
    "\n",
    "class ChatGPTModel(AILanguageModel):\n",
    "    \"\"\"\n",
    "    OpenAI ChatGPT-specific implementation with streaming support\n",
    "    \n",
    "    Args:\n",
    "        model (str): Name of the OpenAI model (e.g., \"gpt-4\", \"gpt-3.5-turbo\")\n",
    "        api_key (str): OpenAI API key\n",
    "        stream (bool): Whether to stream responses (default: True)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str, api_key: str, stream: bool = True):\n",
    "        super().__init__(model, stream)\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "\n",
    "    def _send_to_model(self, messages: List[Dict]) -> Union[str, Generator[str, None, None]]:\n",
    "        if self.stream:\n",
    "            return self._stream_response(messages)\n",
    "        else:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "    \n",
    "    def _stream_response(self, messages: List[Dict]) -> Generator[str, None, None]:\n",
    "        #Stream response from OpenAI model\n",
    "        stream = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        for chunk in stream:\n",
    "            if chunk.choices and chunk.choices[0].delta.content is not None:\n",
    "                yield chunk.choices[0].delta.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "db3bb850-406f-4d35-88ae-8fc5fb03b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = OllamaModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "80d7fb0b-6469-4ce4-883a-fb30c6f4d02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is a simple \"Hello, World!\" program in Python:\n",
       "\n",
       "**hello.py**\n",
       "```python\n",
       "print(\"Hello, World!\")\n",
       "```\n",
       "To run this code, save it to a file called `hello.py` and then open a terminal or command prompt, navigate to the directory where you saved the file, and type:\n",
       "```bash\n",
       "python hello.py\n",
       "```\n",
       "This should print \"Hello, World!\" to the console.\n",
       "\n",
       "Alternatively, if you're using an IDE (Integrated Development Environment) like PyCharm, Visual Studio Code, or Spyder, you can create a new Python project, paste this code into a file called `hello.py`, and run it from within the IDE."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat.send_message(\"can you make me hello world in python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e3a00718-97b2-4c26-9b8d-464c92778e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is an updated version of the \"Hello, World!\" program with an additional function that generates a random integer:\n",
       "\n",
       "**random_hello.py**\n",
       "```python\n",
       "import random\n",
       "\n",
       "def hello_world():\n",
       "    print(\"Hello, World!\")\n",
       "\n",
       "def generate_random_number(min_val=1, max_val=100):\n",
       "    return random.randint(min_val, max_val)\n",
       "\n",
       "hello_world()\n",
       "print(\"Random number:\", generate_random_number())\n",
       "```\n",
       "This code defines two functions:\n",
       "\n",
       "* `hello_world()`: prints \"Hello, World!\" to the console\n",
       "* `generate_random_number()`: generates a random integer between `min_val` and `max_val` (defaulting to 1-100 if no arguments are provided)\n",
       "\n",
       "The code then calls both functions in sequence. The output should look something like this:\n",
       "```\n",
       "Hello, World!\n",
       "Random number: 42\n",
       "```\n",
       "Note that the actual random number will be different each time you run the program!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat.send_message(\"can you make me also code to generate a random number in the same language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6d68422b-797e-4b95-a9bb-ad502bbd22b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'can you make me hello world in python',\n",
       "  'timestamp': '2024-12-20T12:09:29.287424'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Here is a simple \"Hello, World!\" program in Python:\\n\\n**hello.py**\\n```python\\nprint(\"Hello, World!\")\\n```\\nTo run this code, save it to a file called `hello.py` and then open a terminal or command prompt, navigate to the directory where you saved the file, and type:\\n```bash\\npython hello.py\\n```\\nThis should print \"Hello, World!\" to the console.\\n\\nAlternatively, if you\\'re using an IDE (Integrated Development Environment) like PyCharm, Visual Studio Code, or Spyder, you can create a new Python project, paste this code into a file called `hello.py`, and run it from within the IDE.',\n",
       "  'timestamp': '2024-12-20T12:09:32.223662'}]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "92747222-71e8-4db4-882f-00fb688029a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python hello.py', 'print(\"Hello, World!\")']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.code_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7aebab-014b-4ed7-b620-ce208a3cc60d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
